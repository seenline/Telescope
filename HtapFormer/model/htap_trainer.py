"""
HtapFormer Trainer

æ‰©å±• trainer.py ä»¥æ”¯æŒ HTAP ç‰¹å¾ã€‚
ä¸»è¦æ”¹åŠ¨ï¼š
1. åœ¨è®­ç»ƒå’Œè¯„ä¼°æ—¶ä¼ é€’ htap_data
2. æ”¯æŒæ¨¡æ‹Ÿ HTAP ç‰¹å¾ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
3. å‘åå…¼å®¹ï¼šå¦‚æœ htap_data=Noneï¼Œç­‰åŒäº QueryFormer

Author: Research Assistant
Date: 2024
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from model.dataset import PlanTreeDataset
from model.database_util import collator, get_job_table_sample
import os
import time
import torch
from scipy.stats import pearsonr


def create_mock_htap_data(batch_size, n_nodes, device='cpu'):
    """
    åˆ›å»ºæ¨¡æ‹Ÿçš„ HTAP ç‰¹å¾ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
    
    âš ï¸  è¿™æ˜¯ä¸´æ—¶æ–¹æ¡ˆï¼å®é™…ä½¿ç”¨æ—¶éœ€è¦æå–çœŸå®ç‰¹å¾ã€‚
    
    Args:
        batch_size: batch å¤§å°
        n_nodes: æ¯ä¸ªæŸ¥è¯¢çš„èŠ‚ç‚¹æ•°
        device: è®¾å¤‡ (cuda/cpu)
    
    Returns:
        htap_data: æ¨¡æ‹Ÿçš„ HTAP ç‰¹å¾å­—å…¸
    """
    htap_data = {
        # å­˜å‚¨ç±»å‹ï¼šéšæœºåˆ†é…è¡Œå­˜/åˆ—å­˜
        'storage_type': torch.randint(0, 2, (batch_size, n_nodes), device=device),
        
        # æ“ä½œé¢‘ç‡ï¼šæ¨¡æ‹Ÿä¸­ç­‰å†™è´Ÿè½½
        'insert_cnt': torch.rand(batch_size, n_nodes, device=device) * 0.1,
        'update_cnt': torch.rand(batch_size, n_nodes, device=device) * 0.2,
        'delete_cnt': torch.rand(batch_size, n_nodes, device=device) * 0.05,
        
        # æŸ¥è¯¢ç±»å‹ï¼šå‡è®¾å¤§éƒ¨åˆ†æ˜¯ SELECT (3)
        'query_type': torch.full((batch_size, n_nodes), 3, device=device, dtype=torch.long),
        
        # èŠ‚ç‚¹æ“ä½œç¬¦ï¼šéšæœºåˆ†é…
        'node_operator': torch.randint(0, 20, (batch_size, n_nodes), device=device)
    }
    return htap_data


def chunks(l, n):
    """Yield successive n-sized chunks from l."""
    for i in range(0, len(l), n):
        yield l[i:i + n]


def print_qerror(preds_unnorm, labels_unnorm, prints=False):
    qerror = []
    for i in range(len(preds_unnorm)):
        if preds_unnorm[i] > float(labels_unnorm[i]):
            qerror.append(preds_unnorm[i] / float(labels_unnorm[i]))
        else:
            qerror.append(float(labels_unnorm[i]) / float(preds_unnorm[i]))

    e_50, e_90 = np.median(qerror), np.percentile(qerror, 90)    
    e_mean = np.mean(qerror)

    if prints:
        print("Median: {}".format(e_50))
        print("Mean: {}".format(e_mean))

    res = {
        'q_median': e_50,
        'q_90': e_90,
        'q_mean': e_mean,
    }

    return res


def get_corr(ps, ls):  # unnormalised
    ps = np.array(ps)
    ls = np.array(ls)
    corr, _ = pearsonr(np.log(ps), np.log(ls))
    
    return corr


def evaluate_htapformer(model, ds, bs, norm, device, prints=False, use_htap=True):
    """
    HtapFormer è¯„ä¼°å‡½æ•°
    
    Args:
        model: HtapFormer æ¨¡å‹
        ds: æ•°æ®é›†
        bs: batch size
        norm: Normalizer
        device: è®¾å¤‡
        prints: æ˜¯å¦æ‰“å°ç»“æœ
        use_htap: æ˜¯å¦ä½¿ç”¨ HTAP ç‰¹å¾ï¼ˆFalse æ—¶ç­‰åŒäº QueryFormerï¼‰
    
    Returns:
        scores: è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    model.eval()
    cost_predss = np.empty(0)

    with torch.no_grad():
        for i in range(0, len(ds), bs):
            batch, batch_labels = collator(list(zip(*[ds[j] for j in range(i, min(i+bs, len(ds)))])))
            batch = batch.to(device)

            # ğŸŒŸ å…³é”®æ”¹åŠ¨ï¼šæ·»åŠ  HTAP ç‰¹å¾
            if use_htap:
                # è·å– batch å¤§å°å’ŒèŠ‚ç‚¹æ•°
                batch_size = batch.x.size(0)
                n_nodes = batch.x.size(1)
                
                # åˆ›å»ºæ¨¡æ‹Ÿ HTAP ç‰¹å¾
                htap_data = create_mock_htap_data(batch_size, n_nodes, device)
                
                # è°ƒç”¨æ¨¡å‹ï¼ˆä¼ é€’ HTAP ç‰¹å¾ï¼‰
                cost_preds, _ = model(batch, htap_data)
            else:
                # ä¸ä½¿ç”¨ HTAP ç‰¹å¾ï¼ˆç­‰åŒäº QueryFormerï¼‰
                cost_preds, _ = model(batch, htap_data=None)
            
            cost_preds = cost_preds.squeeze()
            cost_predss = np.append(cost_predss, cost_preds.cpu().detach().numpy())
    
    scores = print_qerror(norm.unnormalize_labels(cost_predss), ds.costs, prints)
    corr = get_corr(norm.unnormalize_labels(cost_predss), ds.costs)
    
    if prints:
        print("Corr: {}".format(corr))
    
    scores['corr'] = corr
    return scores


def eval_workload_htapformer(workload, methods, use_htap=True):
    """
    åœ¨æŒ‡å®šå·¥ä½œè´Ÿè½½ä¸Šè¯„ä¼° HtapFormer
    
    Args:
        workload: å·¥ä½œè´Ÿè½½åç§° (e.g., 'job-light', 'synthetic')
        methods: æ–¹æ³•å­—å…¸
        use_htap: æ˜¯å¦ä½¿ç”¨ HTAP ç‰¹å¾
    
    Returns:
        eval_score: è¯„ä¼°åˆ†æ•°
        ds: æ•°æ®é›†
    """
    get_table_sample = methods['get_sample']

    workload_file_name = './data/imdb/workloads/' + workload
    table_sample = get_table_sample(workload_file_name)
    plan_df = pd.read_csv('./data/imdb/{}_plan.csv'.format(workload))
    workload_csv = pd.read_csv('./data/imdb/workloads/{}.csv'.format(workload), sep='#', header=None)
    workload_csv.columns = ['table', 'join', 'predicate', 'cardinality']
    
    ds = PlanTreeDataset(
        plan_df, workload_csv,
        methods['encoding'], methods['hist_file'], 
        methods['cost_norm'], methods['cost_norm'], 
        'cost', table_sample
    )

    eval_score = evaluate_htapformer(
        methods['model'], ds, methods['bs'], 
        methods['cost_norm'], methods['device'], 
        prints=True, use_htap=use_htap
    )
    
    return eval_score, ds


def train_htapformer(model, train_ds, val_ds, crit, norm, args, use_htap=True):
    """
    HtapFormer è®­ç»ƒå‡½æ•°
    
    Args:
        model: HtapFormer æ¨¡å‹
        train_ds: è®­ç»ƒæ•°æ®é›†
        val_ds: éªŒè¯æ•°æ®é›†
        crit: æŸå¤±å‡½æ•°
        norm: Normalizer
        args: è®­ç»ƒå‚æ•°
        use_htap: æ˜¯å¦ä½¿ç”¨ HTAP ç‰¹å¾
    
    Returns:
        model: è®­ç»ƒåçš„æ¨¡å‹
        best_path: æœ€ä½³æ¨¡å‹è·¯å¾„
    """
    import torch.optim as optim
    from torch.utils.data import DataLoader
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    opt = optim.Adam(model.parameters(), lr=args.lr)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = optim.lr_scheduler.StepLR(opt, step_size=1, gamma=args.sch_decay)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_ds, 
        batch_size=args.bs, 
        shuffle=True,
        collate_fn=lambda x: collator(list(zip(*x)))
    )
    
    best_val_loss = float('inf')
    best_epoch = 0
    best_path = None
    
    print(f"{'='*60}")
    print(f"Training HtapFormer")
    print(f"  Use HTAP features: {use_htap}")
    print(f"  HTAP lambda: {getattr(args, 'htap_lambda', 'N/A')}")
    print(f"  Training samples: {len(train_ds)}")
    print(f"  Validation samples: {len(val_ds)}")
    print(f"  Batch size: {args.bs}")
    print(f"  Epochs: {args.epochs}")
    print(f"  Learning rate: {args.lr}")
    print(f"{'='*60}\n")
    
    for epoch in range(args.epochs):
        model.train()
        epoch_loss = 0
        num_batches = 0
        start_time = time.time()
        
        for batch_idx, (batch, batch_labels) in enumerate(train_loader):
            batch = batch.to(args.device)
            
            # è§£åŒ… batch_labels (æ¥è‡ª collator çš„æ ¼å¼)
            # batch_labels æ˜¯ tuple çš„ list: [(cost1, card1), (cost2, card2), ...]
            l, r = zip(*batch_labels)
            batch_cost_label = torch.FloatTensor(l).to(args.device)
            
            # ğŸŒŸ å…³é”®æ”¹åŠ¨ï¼šæ·»åŠ  HTAP ç‰¹å¾
            if use_htap:
                batch_size = batch.x.size(0)
                n_nodes = batch.x.size(1)
                htap_data = create_mock_htap_data(batch_size, n_nodes, args.device)
                cost_preds, _ = model(batch, htap_data)
            else:
                cost_preds, _ = model(batch, htap_data=None)
            
            # è®¡ç®—æŸå¤±
            cost_preds = cost_preds.squeeze()
            loss = crit(cost_preds, batch_cost_label)
            
            # åå‘ä¼ æ’­
            opt.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_size)
            
            opt.step()
            
            # æ¸…ç†å†…å­˜ï¼ˆé˜²æ­¢ CUDA OOMï¼‰
            del batch
            del batch_labels
            if args.device.startswith('cuda'):
                torch.cuda.empty_cache()
            
            epoch_loss += loss.item()
            num_batches += 1
            
            # å®šæœŸæ‰“å°è¿›åº¦
            if batch_idx % 50 == 0:
                print(f"  Epoch {epoch+1}/{args.epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}")
        
        # å¹³å‡è®­ç»ƒæŸå¤±
        avg_train_loss = epoch_loss / num_batches
        epoch_time = time.time() - start_time
        
        # éªŒè¯
        val_score = evaluate_htapformer(
            model, val_ds, args.bs, norm, 
            args.device, prints=False, use_htap=use_htap
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        
        # æ‰“å° epoch ç»“æœ
        print(f"\nEpoch {epoch+1}/{args.epochs} Summary:")
        print(f"  Train Loss: {avg_train_loss:.4f}")
        print(f"  Val Q-Median: {val_score['q_median']:.2f}")
        print(f"  Val Q-Mean: {val_score['q_mean']:.2f}")
        print(f"  Val Correlation: {val_score['corr']:.4f}")
        print(f"  Learning Rate: {current_lr:.6f}")
        print(f"  Epoch Time: {epoch_time:.2f}s\n")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_score['q_median'] < best_val_loss:
            best_val_loss = val_score['q_median']
            best_epoch = epoch + 1
            
            # ä¿å­˜æ¨¡å‹
            model_hash = hash(time.time())
            best_path = os.path.join(args.newpath, f'{model_hash}.pt')
            torch.save(model.state_dict(), best_path)
            
            print(f"  âœ… Best model saved! Q-Median: {best_val_loss:.2f} at epoch {best_epoch}")
            print(f"     Path: {best_path}\n")
        
        # è®°å½•åˆ°æ—¥å¿—
        logging(args, epoch + 1, val_score, filename='log.txt', save_model=False)
    
    print(f"\n{'='*60}")
    print(f"Training Completed!")
    print(f"  Best Epoch: {best_epoch}")
    print(f"  Best Val Q-Median: {best_val_loss:.2f}")
    print(f"  Best Model: {best_path}")
    print(f"{'='*60}\n")
    
    return model, best_path


def logging(args, epoch, test_scores, filename='log.txt', save_model=True, model=None):
    """
    è®°å½•è®­ç»ƒæ—¥å¿—
    
    Args:
        args: è®­ç»ƒå‚æ•°
        epoch: å½“å‰ epoch
        test_scores: æµ‹è¯•åˆ†æ•°
        filename: æ—¥å¿—æ–‡ä»¶å
        save_model: æ˜¯å¦ä¿å­˜æ¨¡å‹
        model: æ¨¡å‹ï¼ˆå¦‚æœ save_model=Trueï¼‰
    
    Returns:
        best_model_path: æœ€ä½³æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœä¿å­˜ï¼‰
    """
    logfile = os.path.join(args.newpath, filename)
    
    res = {
        'epoch': epoch,
        **test_scores
    }
    
    # è¯»å–ç°æœ‰æ—¥å¿—
    if os.path.exists(logfile):
        df = pd.read_csv(logfile)
    else:
        df = pd.DataFrame()
    
    # æ·»åŠ æ–°è®°å½•
    res_df = pd.DataFrame([res])
    df = pd.concat([df, res_df], ignore_index=True)
    
    # ä¿å­˜æ—¥å¿—
    df.to_csv(logfile, index=False)
    
    # ä¿å­˜æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
    best_model_path = None
    if save_model and model is not None:
        model_hash = hash(time.time())
        best_model_path = os.path.join(args.newpath, f'{model_hash}.pt')
        torch.save(model.state_dict(), best_model_path)
    
    return best_model_path

